{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Содержание<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Подготовка\" data-toc-modified-id=\"Подготовка-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Подготовка</a></span></li><li><span><a href=\"#Обучение\" data-toc-modified-id=\"Обучение-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Обучение</a></span></li><li><span><a href=\"#Выводы\" data-toc-modified-id=\"Выводы-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Выводы</a></span></li><li><span><a href=\"#Чек-лист-проверки\" data-toc-modified-id=\"Чек-лист-проверки-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Чек-лист проверки</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Проект модели классификации позитивных и негативных комментариев"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Интернет-магазин запускает новый сервис. Теперь пользователи могут редактировать и дополнять описания товаров, как в вики-сообществах. Клиенты предлагают свои правки и комментируют изменения других. \n",
    "\n",
    "**Задача:** найти инструмент, который будет искать токсичные комментарии и отправлять их на модерацию.\n",
    "\n",
    "**Цель:** найти модель классификации комментариев на позитивные и негативные со значением метрики качества F1 >= 0.75.\n",
    "\n",
    "**План:**\n",
    "\n",
    "- Обзор данных.\n",
    "- Подготовка данных.\n",
    "- Обучение моделей.\n",
    "\n",
    "**Описание данных**\n",
    "\n",
    "Файл - `toxic_comments.csv`. \n",
    "- Столбец *text* - текст комментария.\n",
    "- Столбец *toxic* — целевой признак."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Подготовка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords as nltk_stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score, roc_auc_score, roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.pipeline import Pipeline\n",
    "from lightgbm import LGBMClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    data = pd.read_csv('toxic_comments.csv', index_col=0)\n",
    "except:\n",
    "    data = pd.read_csv('/datasets/toxic_comments.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "<font size=\"5\"><b>Комментарий ревьюера</b></font>\n",
    "\n",
    "Совет: \n",
    "\n",
    "\n",
    "Если не знаешь - чтобы не было столбца  `Unnamed: 0` при чтении файла можно так:\n",
    "\n",
    "\n",
    "    pd.read_csv(..., index_col=0)\n",
    "\n",
    "    \n",
    "(`Unnamed: 0` появляется при не совсем корректном сохранении файла)    \n",
    "\n",
    "<div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 159292 entries, 0 to 159450\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count   Dtype \n",
      "---  ------  --------------   ----- \n",
      " 0   text    159292 non-null  object\n",
      " 1   toxic   159292 non-null  int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 3.6+ MB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>966</th>\n",
       "      <td>\"Just to add Kansas Bear.When I said:\\n\\n\"\"You...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84755</th>\n",
       "      <td>edit again so I will have to</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49509</th>\n",
       "      <td>Re:Move\\nI read the arguments and judged that ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144456</th>\n",
       "      <td>Apology \\n\\nOk, After a chance meeting with ag...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88140</th>\n",
       "      <td>cited material censorship \\nwhy, duncharris an...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135735</th>\n",
       "      <td>June 28, 2005 23:23 (UTC)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113678</th>\n",
       "      <td>\" (UTC)\\n\\n If one takes that attitude, it nev...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53146</th>\n",
       "      <td>please reffer tkz.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58752</th>\n",
       "      <td>That probably is because you haven't specified...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103639</th>\n",
       "      <td>Fuck's sake, I didn't even edit the article!  ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>404</th>\n",
       "      <td>Hi Collectonian, it obviously wasn't appropria...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106293</th>\n",
       "      <td>\"\\nWelcome!\\n\\nHello, , and welcome to Wikiped...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138985</th>\n",
       "      <td>I am God and Good\\nI'm the new master of wikim...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22460</th>\n",
       "      <td>\"\\n\\nBarnstars awarded!\\n\\n  The Random Acts o...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136622</th>\n",
       "      <td>|listas = Mount Baker Hard Core</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text  toxic\n",
       "966     \"Just to add Kansas Bear.When I said:\\n\\n\"\"You...      0\n",
       "84755                        edit again so I will have to      0\n",
       "49509   Re:Move\\nI read the arguments and judged that ...      0\n",
       "144456  Apology \\n\\nOk, After a chance meeting with ag...      0\n",
       "88140   cited material censorship \\nwhy, duncharris an...      0\n",
       "135735                          June 28, 2005 23:23 (UTC)      0\n",
       "113678  \" (UTC)\\n\\n If one takes that attitude, it nev...      0\n",
       "53146                               please reffer tkz.jpg      0\n",
       "58752   That probably is because you haven't specified...      0\n",
       "103639  Fuck's sake, I didn't even edit the article!  ...      1\n",
       "404     Hi Collectonian, it obviously wasn't appropria...      0\n",
       "106293  \"\\nWelcome!\\n\\nHello, , and welcome to Wikiped...      0\n",
       "138985  I am God and Good\\nI'm the new master of wikim...      0\n",
       "22460   \"\\n\\nBarnstars awarded!\\n\\n  The Random Acts o...      0\n",
       "136622                    |listas = Mount Baker Hard Core      0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.info()\n",
    "data.sample(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Поверим на пропуски и дубликаты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Дубликатов: 0\n",
      "Пропусков: text     0\n",
      "toxic    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print('Дубликатов:', data.duplicated().sum())\n",
    "print('Пропусков:', data.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В данных 159 292 объекта. Пропусков нет, дубликатов нет."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим сколько токсичных и не токсичных комментариев"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    143106\n",
       "1     16186\n",
       "Name: toxic, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "8.841344371679229"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display(data['toxic'].value_counts())\n",
    "\n",
    "ratio = data['toxic'].value_counts()[0] / data['toxic'].value_counts()[1]\n",
    "ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В целевом признаке 90% объектов отрицательного класса."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<font size=\"5\"><b>Комментарий ревьюера</b></font>\n",
    "\n",
    "Успех:\n",
    "\n",
    "Данные изучены. Небольшой EDA не помешает, так как это аналитический проект. \n",
    "\n",
    "\n",
    "Плюс за\n",
    "\n",
    "    \n",
    "\n",
    "-  проверку на сбалансированность \n",
    "\n",
    "\n",
    "\n",
    "- Промежуточный вывод в конце раздела\n",
    "\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Совет: \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "- .sample() вместо .head(), ведь если данные каким то образом упорядоченны, то шансы увидеть что то разнообразное через .sample чуть выше чем через .head (или .tail)     \n",
    "   \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<font size=\"2\"><b>Комментарий студента</b></font>\n",
    "\n",
    "Готово, спасибо!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<font size=\"5\"><b>Комментарий ревьюераV2</b></font>\n",
    "\n",
    "\n",
    "\n",
    "Успех 👍:\n",
    "\n",
    "\n",
    "\n",
    "👍\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Очистим тексты комментариев и леммализируем"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    text = text.lower()\n",
    "    lemm_text = \"\".join(lemmatizer.lemmatize(text))\n",
    "    cleared_text = re.sub(r'[^a-zA-Z]', ' ', lemm_text) \n",
    "    return \" \".join(cleared_text.split())\n",
    "\n",
    "data['lemm_text'] = data['text'].apply(lemmatize_text)\n",
    "\n",
    "data = data.drop(['text'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\">\n",
    "<font size=\"5\"><b>Комментарий ревьюера</b></font>\n",
    "\n",
    "Ошибка:\n",
    "\n",
    "\n",
    "\n",
    "- WordNetLemmatizer  рабочий вариант, но у него есть особенности, для корректной работы ему нужно передавать не просто слово, но и POS-тег (Part of Speech, части речи). Набираемся ума-разума [тут](https://webdevblog.ru/podhody-lemmatizacii-s-primerami-v-python/) )  Обрати внимание на функцию `get_wordnet_pos`\n",
    "\n",
    "\n",
    "- Что насчет токенизации? Лемматизацию мы применяем к словам, а не предложениям, WordNetLemmatizer с предложениями не работает.\n",
    "\n",
    "\n",
    "\n",
    "И сразу предупрежу что процесс лематизации затянется на полчаса - час. Ниже помог кодом\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "\n",
    "Совет: \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "- все это можно было сделать с помощью SpaCy лемматизатором и прямо скажем как инструмент он более удобен и универсален, можно поподробней тут [почитать](https://habr.com/ru/post/531940/)  Хотя и более медленный  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<font size=\"2\"><b>Комментарий студента</b></font>\n",
    "\n",
    "Спасибо!! И за ссылки спасибо! Применим функцию и используем WordNetLemmatizer используем с POS#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим функцию для очистки данных, слов и лемматизируем каждое слово"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = list(data['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk.corpus import wordnet\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def clear_text(text):\n",
    "    y=re.sub(r\"[^'a-zA-Z ]\", ' ', text) \n",
    "    k=\" \".join(y.split())\n",
    "    return k\n",
    "\n",
    "from nltk.corpus import wordnet\n",
    "def get_wordnet_pos(word):\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "    return tag_dict.get(tag, wordnet.NOUN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверим работу"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'strip', 'bat', 'be', 'hang', 'on', 'their', 'foot', 'for', 'best']\n"
     ]
    }
   ],
   "source": [
    "text = \"The striped bats are hanging on their feet for best\"\n",
    "print([lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in nltk.word_tokenize(text)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The strip bat be hang on their foot for best\n"
     ]
    }
   ],
   "source": [
    "print(' '.join([lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in nltk.word_tokenize(text)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemm_(text):\n",
    "\n",
    "    lemmatized_output = ' '.join([lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in nltk.word_tokenize(text)])     \n",
    "    return lemmatized_output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "было\n"
     ]
    }
   ],
   "source": [
    "print('было')\n",
    "sentence1 = \"The striped bats are hanging on their feet for best\"\n",
    "sentence2 = \"you should be ashamed of yourself went worked\"\n",
    "df_my = pd.DataFrame([sentence1, sentence2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "стало после лемматизации\n",
      "0    The strip bat be hang on their foot for best\n",
      "1       you should be ashamed of yourself go work\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print('стало после лемматизации')\n",
    "print(df_my[0].apply(lemm_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Применим к нашим данным"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae76d696716740af89972057b93823e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/159292 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 19min 27s, sys: 1min 53s, total: 21min 20s\n",
      "Wall time: 21min 52s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def lemmafunction(text):\n",
    "    k=[]\n",
    "    for i in nltk.word_tokenize(text):\n",
    "        y=lemmatizer.lemmatize(i, get_wordnet_pos(i))\n",
    "        k.append(y)\n",
    "    return ' '.join(k) \n",
    "\n",
    "lemy=[]\n",
    "for i in tqdm(range(len(corpus))):\n",
    "    \n",
    "    lemy.append(lemmafunction(clear_text(corpus[i])))\n",
    "data['lemm_text']=pd.Series(lemy, index=data.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<font size=\"5\"><b>Комментарий ревьюераV2</b></font>\n",
    "\n",
    "\n",
    "\n",
    "Успех 👍:\n",
    "\n",
    "\n",
    "\n",
    "Принято.  Но хочу заметить что циклы и Тоне это не самый эффективный вариант.  Использование apply было бы лучше \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hey man, I'm really not trying to edit war. It's just that this guy is constantly removing relevant information and talking to me through edits instead of my talk page. He seems to care more about the formatting than the actual info.\""
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['text'][2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Преобразуем текст"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hey man I 'm really not try to edit war It 's just that this guy be constantly remove relevant information and talk to me through edits instead of my talk page He seem to care more about the format than the actual info\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['lemm_text'][2]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "WordNetLemmatizer используем с POS# код ревьюера\n",
    "\n",
    "\n",
    "\n",
    "# вот код из статьи:\n",
    "\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk.corpus import wordnet\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "\n",
    "# [lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in nltk.word_tokenize(text)]\n",
    "\n",
    "# text это предложение, которое с помощью  nltk.word_tokenize разбивается на слова (w), а эти слова подаются в \n",
    "# lemmatizer.lemmatize(w, get_wordnet_pos(w)) и происходит лемматизация. Давай попробуем:\n",
    "\n",
    "text = \"The striped bats are hanging on their feet for best\"\n",
    "print([lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in nltk.word_tokenize(text)])\n",
    "\n",
    "\n",
    "# striped  ----->  strip  hanging------> hang  итп итд, лемматизация произошла!\n",
    "\n",
    "# А теперь давай слепим это в предложение:\n",
    "    \n",
    "print(' '.join([lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in nltk.word_tokenize(text)])    )\n",
    "\n",
    "# Красота! Все работает\n",
    "\n",
    "\n",
    "# а теперь просто нужно чуть подправить функцию:\n",
    "\n",
    "    \n",
    "def lemm_(text):\n",
    "\n",
    "    lemmatized_output = ' '.join([lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in nltk.word_tokenize(text)])     \n",
    "    return lemmatized_output    \n",
    "\n",
    "\n",
    "\n",
    "# проверим. создадим датафрейм из двух строк:\n",
    "print('было')\n",
    "sentence1 = \"The striped bats are hanging on their feet for best\"\n",
    "sentence2 = \"you should be ashamed of yourself went worked\"\n",
    "df_my = pd.DataFrame([sentence1, sentence2])\n",
    "\n",
    "print(df_my)\n",
    "\n",
    "\n",
    "print('стало после лемматизации')\n",
    "# и попробуем нашу функцию c помощью apply\n",
    "print(df_my[0].apply(lemm_))\n",
    "\n",
    "# вуаля!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>lemm_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>Explanation Why the edits make under my userna...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>D'aww He match this background colour I 'm see...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>Hey man I 'm really not try to edit war It 's ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>More I ca n't make any real suggestion on impr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>You sir be my hero Any chance you remember wha...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  toxic  \\\n",
       "0  Explanation\\nWhy the edits made under my usern...      0   \n",
       "1  D'aww! He matches this background colour I'm s...      0   \n",
       "2  Hey man, I'm really not trying to edit war. It...      0   \n",
       "3  \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
       "4  You, sir, are my hero. Any chance you remember...      0   \n",
       "\n",
       "                                           lemm_text  \n",
       "0  Explanation Why the edits make under my userna...  \n",
       "1  D'aww He match this background colour I 'm see...  \n",
       "2  Hey man I 'm really not try to edit war It 's ...  \n",
       "3  More I ca n't make any real suggestion on impr...  \n",
       "4  You sir be my hero Any chance you remember wha...  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<font size=\"5\"><b>Комментарий ревьюера</b></font>\n",
    "\n",
    "Успех:\n",
    "\n",
    "\n",
    "- Плюс за использование apply, неэффективные циклы нам ни к чему.\n",
    "\n",
    "\n",
    "- Да, всегда лучше проверить что получилось  в итоге, так всегда будет возможность поправить ошибку\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "\n",
    "Совет: \n",
    "\n",
    "\n",
    "    \n",
    "- попробуй .progress_apply, делает что .apply, но еще и показывает на какой итерации находится процесс.  \n",
    "\n",
    "Для некоторых версий, чтобы заработал.progress_apply предварительно нужно сделать:\n",
    "    \n",
    "    \n",
    "    from tqdm.notebook import tqdm\n",
    "    tqdm.pandas()\n",
    "    \n",
    "\n",
    "А если делаешь Colab и процесс лемматизации и очистки затягивается, попробуй  .parallel_apply,  кому-то это помогает уменьшить время прогона кода раз в 5-7. Предварительно: \n",
    "\n",
    "\n",
    "    \n",
    "    from pandarallel import pandarallel   \n",
    "    tqdm.pandas(desc=\"progress\")\n",
    "    pandarallel.initialize(progress_bar = True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "- после очистки и лемматизации (и убрав стопслова) можно провести частотный анализ текста/[облако слов](https://habr.com/ru/post/517410/) - чтобы получить общее представление о тематике и о наиболее часто встречаемых словах в токсичных и нетоксичных твитах Кроме того графики, рисунки делают проект визуально интересней\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<font size=\"2\"><b>Комментарий студента</b></font>\n",
    "\n",
    "Готово, С tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разделим выборки. По пропорциям в соответствии распределению по классам в target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "features=data.drop('toxic',axis=1)\n",
    "target=data['toxic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train, features_test, target_train, target_test = train_test_split(features, target, test_size=0.25, random_state=42, shuffle=True,stratify=target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<font size=\"5\"><b>Комментарий ревьюера</b></font>\n",
    "\n",
    "Успех:\n",
    "\n",
    "\n",
    "- random_state на месте\n",
    "\n",
    "\n",
    "\n",
    "- здорово что используешь stratify    \n",
    "\n",
    "\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "\n",
    "\n",
    "Совет: \n",
    "\n",
    "\n",
    "- если мы используем вручную прописаный цикл, то нам ещё нужен валидационный датасет. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(119469, 2)\n",
      "(39823, 2)\n",
      "(119469,)\n",
      "(39823,)\n"
     ]
    }
   ],
   "source": [
    "print(features_train.shape)\n",
    "print(features_test.shape)\n",
    "print(target_train.shape)\n",
    "print(target_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размер тренировочной выборки- 75%\n",
      "Размер тестовой выборки - 25%\n"
     ]
    }
   ],
   "source": [
    "train_sample=features_train.shape[0]/features.shape[0]\n",
    "test_sample=features_test.shape[0]/features.shape[0]\n",
    "\n",
    "print('Размер тренировочной выборки- {:.0%}'.format(train_sample))\n",
    "print('Размер тестовой выборки - {:.0%}'.format(test_sample))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Распределение класов в тренировочной выборке"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    0.898384\n",
      "1    0.101616\n",
      "Name: toxic, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "ax = target_train.value_counts(normalize=True)\n",
    "print(ax)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## ax = target_train.value_counts().plot(kind='bar', figsize=(7, 5),\n",
    "                                                  color=\"indigo\", fontsize=15);\n",
    "ax.set_alpha(0.9)\n",
    "ax.set_title(\"Распределение классов\", fontsize=12)\n",
    "ax.set_ylabel(\"Количество\", fontsize=12);\n",
    "\n",
    "totals = []\n",
    "\n",
    "for i in ax.patches:\n",
    "    totals.append(i.get_height())\n",
    "\n",
    "total = sum(totals)\n",
    "\n",
    "for i in ax.patches:\n",
    "    ax.text(i.get_x()+.17, i.get_height()-10000,  \\\n",
    "            str(round((i.get_height()/total)*100))+'%', fontsize=15,\n",
    "                color='white')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## def down_up_sample(features, target, fraction, repeat):\n",
    "    features_zeros = features[target == 0]\n",
    "    features_ones = features[target == 1]\n",
    "    target_zeros = target[target == 0]\n",
    "    target_ones = target[target == 1]\n",
    "\n",
    "    features_down_up = pd.concat([features_zeros.sample(frac=fraction, random_state=12345)] + [features_ones]*repeat)\n",
    "    target_down_up = pd.concat([target_zeros.sample(frac=fraction, random_state=12345)] + [target_ones]*repeat)\n",
    "    \n",
    "    features_down_up_sampled, target_down_up_sampled = shuffle(\n",
    "        features_down_up, target_down_up, random_state=12345)\n",
    "    \n",
    "    return features_down_up_sampled, target_down_up_sampled\n",
    "\n",
    "features_upsampled, target_upsampled  = down_up_sample(features_train, target_train, 0.5, 5)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## ax = target_upsampled.value_counts(normalize=True)\n",
    "print(ax)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## ax = target_upsampled.value_counts().plot(kind='bar', figsize=(7, 5),\n",
    "                                                  color=\"indigo\", fontsize=15);\n",
    "ax.set_alpha(0.9)\n",
    "ax.set_title(\"Распределение классов\", fontsize=12)\n",
    "ax.set_ylabel(\"Количество\", fontsize=12);\n",
    "\n",
    "totals = []\n",
    "\n",
    "for i in ax.patches:\n",
    "    totals.append(i.get_height())\n",
    "\n",
    "total = sum(totals)\n",
    "\n",
    "for i in ax.patches:\n",
    "    ax.text(i.get_x()+.17, i.get_height()-10000,  \\\n",
    "            str(round((i.get_height()/total)*100))+'%', fontsize=15,\n",
    "                color='white')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "stopwords = set(nltk_stopwords.words('english'))\n",
    "\n",
    "count_tf_idf = TfidfVectorizer(stop_words=stopwords)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "features_train = count_tf_idf.fit_transform(features_train['lemm_text'].values.astype('U'))\n",
    "features_test = count_tf_idf.transform(features_test['lemm_text'].values.astype('U'))\n",
    "print(features_train.shape)\n",
    "print(features_test.shape)\n",
    "cv_counts = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\">\n",
    "<font size=\"5\"><b>Комментарий ревьюера</b></font>\n",
    "\n",
    "Ошибка:\n",
    "\n",
    "\n",
    "\n",
    "Да, в тренажере был текст на кирилице, там перевод в unicode оправдан. В нашем случае (латиница) это лишь  увеличит количество потребляемой памяти и это в лучшем случаи, в худшем он обрушает ядро.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<font size=\"2\"><b>Комментарий студента</b></font>\n",
    "\n",
    "Да, спасибо, поняла, перевод не оправдан совершенно в данном случае."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<font size=\"5\"><b>Комментарий ревьюераV2</b></font>\n",
    "\n",
    "\n",
    "\n",
    "Успех 👍:\n",
    "\n",
    "\n",
    "\n",
    "экономим ресурсы\n",
    "\n",
    "\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<font size=\"5\"><b>Комментарий ревьюера</b></font>\n",
    "\n",
    "Успех:\n",
    "\n",
    "\n",
    "Не забыли о стопсловах, они ни к чему и код побежит быстрей\n",
    "\n",
    "    \n",
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "\n",
    "Совет:     \n",
    "\n",
    "Вопросик:\n",
    "\n",
    "А стопслова важней убирать  когда мы используем TF-IDF, или когда используе обычный CountVectorizer? \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Итак, подготовили данные для обуения, в том числе:\n",
    "- Поверили на пропуски и дубликаты.\n",
    "- Очистили тексты отзывов и леммализировали.\n",
    "- Получили признаки для обучения, разделили данные на обучающую, тестовую выборки."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<font size=\"5\"><b>Комментарий ревьюера</b></font>\n",
    "\n",
    "\n",
    "\n",
    "Успех 👍:\n",
    "\n",
    "\n",
    "\n",
    "Здорово что в каждом разделе оставляешь промежуточные вывод\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<font size=\"2\"><b>Комментарий студента</b></font>\n",
    "\n",
    "Печально конечно"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "<font size=\"5\"><b>Комментарий ревьюераV2</b></font>\n",
    "\n",
    "\n",
    "\n",
    "Совет 🤔:\n",
    "\n",
    "\n",
    "\n",
    "Боже мой. Я  использую диктовку с голоса, и он иногда такое пишет...Не заметил, тысяча извинений ) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## best_model_dt = None\n",
    "best_result_dt = 0\n",
    "best_depth_dt=0\n",
    "for depth in range(1, 25, 3):\n",
    "      \n",
    "    model_dt = DecisionTreeClassifier(random_state=12345, max_depth=depth,class_weight='balanced') \n",
    "    model_dt.fit(features_train,target_train) \n",
    "    predict_dt=model_dt.predict(features_test) \n",
    "    result_dt = f1_score(target_test, predict_dt)\n",
    "    if result_dt > best_result_dt:\n",
    "        best_model_dt = model_dt\n",
    "        best_result_dt = result_dt \n",
    "          \n",
    "        best_depth_dt = depth\n",
    "print(\"F1 наилучшей модели равно:\", best_result_dt.round(2), end='')\n",
    "print(' C глубиной:',best_depth_dt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\">\n",
    "<font size=\"5\"><b>Комментарий ревьюера</b></font>\n",
    "\n",
    "Ошибка:\n",
    " \n",
    "Во-первых. Работа НЕ выполнена в соответствии с критериями: \n",
    "\n",
    "\n",
    "\n",
    " - модель обучена на обучающем наборе\n",
    " - получена оценка качества на валидационном наборе\n",
    " - перебор гиперпараметров осуществляется в цикле\n",
    "\n",
    "\n",
    "Что именно Догадайся сама )    \n",
    "    \n",
    "    \n",
    "Во-вторых\n",
    "    \n",
    "    \n",
    "Пока не выберем лучшую модель на валидации, тестовой выборки для нас как будто бы не существует    \n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Совет:\n",
    "    \n",
    "    \n",
    "Советую забыть  вручную прописанные цикл и все модели прогшнать используя  GridSearchCV   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<font size=\"2\"><b>Комментарий студента</b></font>\n",
    "\n",
    "Обучим модели LogisticRegression и LightGBM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LogisticRegression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'clf__C': 10, 'clf__class_weight': 'balanced'}\n",
      "0.7785597134181806\n"
     ]
    }
   ],
   "source": [
    "lr_pipe = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(ngram_range=(1,3), min_df=3, max_df=0.9, use_idf=1,\n",
    "               smooth_idf=1, sublinear_tf=1, stop_words=stopwords)),\n",
    "    ('clf', LogisticRegression(random_state=12345))])\n",
    "\n",
    "params = {'clf__C': [0.1, 1, 10, 100],\n",
    "          'clf__class_weight': ['balanced', None]}\n",
    "\n",
    "lr_grid = GridSearchCV(estimator=lr_pipe, param_grid=params, cv=3, scoring='f1', n_jobs=-1, refit=False)\n",
    "lr_grid.fit(features_train['text'], target_train)\n",
    "lr_best_paramms = lr_grid.best_params_\n",
    "\n",
    "print(lr_best_paramms)\n",
    "print(lr_grid.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Итак, лучший best_score метрики 0,77"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<font size=\"5\"><b>Комментарий ревьюераV2</b></font>\n",
    "\n",
    "\n",
    "\n",
    "Успех 👍:\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "То что ты с ручного цикла сразу перескочила на GS+pipeline, это большой прогресс. Теперь всё по фэншую\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "\n",
    "Совет:  \n",
    "\n",
    "\n",
    "\n",
    "На будущее    \n",
    "    \n",
    "    \n",
    "- Совет 1\n",
    "    \n",
    "    \n",
    "Как создавать собственные функции в pipeline (мы пользовались стандартными из sklearn - Scaler, MinMax, или как в этом проекте TFIDF итп)    \n",
    "\n",
    "\n",
    "Можешь взять за основу [Ссылка 1](https://dzen.ru/media/id/5ee6f73b7cadb75a66e4c7e3/sozdanie-polzovatelskih-preobrazovatelei-dannyh-62b2a9a80e49941961ffc7a2),\n",
    "[Ссылка 2](https://towardsdatascience.com/pipelines-custom-transformers-in-scikit-learn-the-step-by-step-guide-with-python-code-4a7d9b068156)\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "- Совет 2  \n",
    "    \n",
    "    \n",
    "\n",
    "Можешь попробовать  feature_engenering (Это когда мы создаём собственные признаки. Во многих случаях это более эффективный способ повысить нашу метрику,  чем долго обучать разные модели) c pipeline:   \n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "1. Сгенерировать новые фичи, например например посчитать число слов в тексте, длину слов, число знаков препинания, число слов с заглавной итп итд. \n",
    "    \n",
    "   \n",
    "    \n",
    "2. Добавить к фичам от векторайзера\n",
    "    \n",
    "    \n",
    "Это можно было реализовать в последующей схеме через [ColumnTransformer](https://scikit-learn.org/stable/modules/generated/sklearn.compose.ColumnTransformer.html):\n",
    "    \n",
    "    \n",
    "    new_features = ['lengh', 'number', ....]\n",
    "    # имена столбцов которых у нас записаны новые фичи (lengh - допустим длина слов твитах, number - количество слов в твите...)\n",
    "    \n",
    "    # lemmatized_text - столбец с текстом\n",
    "    \n",
    "    features = ColumnTransformer(\n",
    "                        [(\"text_preprocess\", TfidfVectorizer(stop_words=stopwords), \"lemmatized_text\"),\n",
    "                         (\"new_features_preprocess\", StandardScaler(), new_features)\n",
    "                        ])\n",
    "\n",
    "    \n",
    "    pipe = Pipeline([('features_all_prepross', features),\n",
    "                     ('model', LogisticRegression(random_state = 42))\n",
    "                    ])\n",
    "\n",
    "Какие именно признаки сгенерировать, это целое искусство. Студент который использовал библиотеку\n",
    "    \n",
    "    \n",
    "    from nltk.sentiment.vader import SentimentIntensityAnalyzer    \n",
    "\n",
    "смог здорово поднять метрику"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<font size=\"5\"><b>Комментарий ревьюера</b></font>\n",
    "\n",
    "Успех:\n",
    "\n",
    "Корректно использован GridSearch \n",
    "\n",
    "\n",
    "\n",
    "- не забыт random_state\n",
    "\n",
    "\n",
    "- class_weight = 'balanced'\n",
    "\n",
    "\n",
    "- scoring = 'f1'\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Совет: \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Молодец что используешь GridSearch, но еще лучше использовать связку GridSearchCV + pipeline. \n",
    "\n",
    "\n",
    "О pipeline:\n",
    "\n",
    "[Pipeline](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html), это тема которая сразу затрагивает кроссвалидацию, тюнинг \"векторайз\", подбор гиперпараметров модели и о том что код стоит делать компактным.\n",
    "    \n",
    "    \n",
    "- в TfidfVectorizer(stop_words=stopwords) у тебя по умолчанию ngram_range=(1, 1), тут можно подбирать разное число n- грамм (и другие параметры), максимизируя метрику, но как объединить перебор по ngram_range с обучением моделей, чтобы не делать это по отдельности или с использованием цикла?! pipeline! Готовый [пример для работы с текстами](https://medium.com/@yoni.levine/how-to-grid-search-with-a-pipeline-93147835d916). Всё что нужно там есть, хотя очень лаконично. Можешь погуглить по:\n",
    "\n",
    "\n",
    "    \n",
    "    pipeline nlp gridsearchcv\n",
    "\n",
    "\n",
    "\n",
    "- как избежать ошибки подглядывания в будущее, когда мы предварительно работаем с данными (шкалирование, нормализация, TfidfVectorizer итп итд)? pipeline! особенно это важно, когда мы используем кроссвалидацию. Для TfidfVectorizer делаем .fit (обучаемся) на train, а transform на test, но точно также нужно сделать для валидационной выборки. Но GS делает валидационные внутри себя, спрашивается как добраться до нее и избежать подглядывания в будущее? Казалось бы никак, но нет! Pipeline! ) \n",
    "    \n",
    "    \n",
    "- pipeline позволяет делать наш код компактней и читабельней, это большой плюс, когда код будет раздуваться     \n",
    "    \n",
    "    \n",
    "\n",
    "         \n",
    "Если раньше не использовала pipeline то могу посоветовать видео в котором [индус](https://www.youtube.com/watch?v=mOYJCR0IDk8&ab_channel=HimanshuChandra) на английском с сильным акцентом, но на пальцах обьясняет  самое непонятное (по моему опыту): сопряженность методов fit и transform. Там же есть и код и ссылка на текст. Мне помогло )\n",
    "\n",
    "\n",
    "\n",
    "В общем если сделать GS+pipeline будет вообще хорошо )  \n",
    "    \n",
    "<div>   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\">\n",
    "<font size=\"5\"><b>Комментарий ревьюера</b></font>\n",
    "\n",
    "\n",
    "\n",
    "Ошибка ❌:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "- Переносим тестирование в самый конец\n",
    "    \n",
    "    \n",
    "- Надо вывести .best_score_ -  Это есть Метрика на валидации когда мы используем GS  \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## best_model_rf = None\n",
    "best_result_rf = 0\n",
    "best_depth_rf=0\n",
    "for est in range(1, 12, 4):\n",
    "      \n",
    "    model_rf = RandomForestClassifier(random_state=12345, n_estimators=est, class_weight='balanced_subsample') \n",
    "    model_rf.fit(features_train,target_train) \n",
    "    predict_rf=model_rf.predict(features_test) \n",
    "    result_rf = f1_score(target_test, predict_rf)\n",
    "    if result_rf > best_result_rf:\n",
    "        best_model_rf = model_rf\n",
    "        best_result_rf = result_rf\n",
    "          \n",
    "        best_depth_rf = est\n",
    "print(\"F1 наилучшей модели равно:\", best_result_rf.round(2), end='')\n",
    "print(' C глубиной:',best_depth_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "<font size=\"5\"><b>Комментарий ревьюера</b></font>\n",
    "\n",
    "\n",
    "\n",
    "Совет 🤔:\n",
    "\n",
    "Советы по выбору модели для экономии времени   \n",
    "\n",
    "\n",
    "- Деревянные модели (RF, DT) медленные, и на данном датасете не показывает хорошие результаты. Лучше используй Логистическую регрессию и LightGBM\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "- Советую сконцентрироваться у Логистической регрессии на переборе \"C\", а max_iter и solver которые частенько используют, лучше не трогать. \n",
    "\n",
    "\n",
    "- Не забываем n_jobs = -1 (Иногда помогает). У LightGBM Можно попробуй явно указать num_threads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LGBMClassifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'clf__learning_rate': 0.25, 'clf__max_depth': -1, 'clf__n_estimators': 50}\n",
      "0.7553530236307816\n"
     ]
    }
   ],
   "source": [
    "lgb_pipe = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(ngram_range=(1,3), min_df=3, max_df=0.9, use_idf=1,\n",
    "               smooth_idf=1, sublinear_tf=1, stop_words=stopwords)),\n",
    "    ('clf', LGBMClassifier(random_state=12345))])\n",
    "\n",
    "params = {\n",
    "  'clf__n_estimators': [50],\n",
    "  'clf__learning_rate': [0.15, 0.25],\n",
    "  'clf__max_depth': [6, 8, -1]}\n",
    "\n",
    "lgb_grid = GridSearchCV(estimator=lgb_pipe, param_grid=params, cv=3, scoring='f1', n_jobs=-1, refit=False)\n",
    "lgb_grid.fit(features_train['text'], target_train)\n",
    "lgb_best_params = lgb_grid.best_params_\n",
    "\n",
    "print(lgb_best_params)\n",
    "print(lgb_grid.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Лучший best_score_ метрики у модели LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF и финальное тестирование лучшей модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorize = TfidfVectorizer(ngram_range=(1,3),\n",
    "               min_df=3, max_df=0.9, use_idf=1,\n",
    "               smooth_idf=1, sublinear_tf=1, stop_words=stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train = vectorize.fit_transform(features_train['text'])\n",
    "features_test = vectorize.transform(features_test['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=10, class_weight='balanced', random_state=12345)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_t = LogisticRegression(C=10, class_weight='balanced', random_state=12345)\n",
    "lr_t.fit(features_train, target_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "<font size=\"5\"><b>Комментарий ревьюера</b></font>\n",
    "    \n",
    "    \n",
    "Совет:\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "Не надо воспринимать  GS как способ получить .best_params_, чтобы подставить их в модель и обучить на них. GS это сделал уже и модельку положил тут: .best_estimator_\n",
    "    \n",
    "  \n",
    "То есть вот это не нужно    \n",
    "    \n",
    "    lr_t = LogisticRegression(C=10, class_weight='balanced', random_state=12345)\n",
    "    lr_t.fit(features_train, target_train)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "lgb_t = LGBMClassifier(learning_rate=0.25, max_depth=-1, n_estimators=200, random_state = 12345)\n",
    "lgb_t.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "## сводная таблица по показателям F1\n",
    "\n",
    "index = ['LogisticRegression',\n",
    "         'DecisionTreeClassifier',\n",
    "         'RandomForestClassifier',\n",
    "        ]\n",
    "\n",
    "data = {'F1': [f1_log_r_1, \n",
    "               f1_forest_1,\n",
    "               f1_random_forest]}\n",
    "\n",
    "f1_data = pd.DataFrame(data=data, index=index)\n",
    "print(f1_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 на тесте: 0.79\n"
     ]
    }
   ],
   "source": [
    "test_pred = lr_t.predict(features_test)\n",
    "test_f1 = f1_score(target_test, test_pred)\n",
    "print('F1 на тесте: {:.2f}'.format(test_f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<font size=\"5\"><b>Комментарий ревьюераv2</b></font>\n",
    "\n",
    "Успех: \n",
    "\n",
    "\n",
    "\n",
    "- Если студент получил на тесте f1 выше 0,75, это считается приемлемым результатом.\n",
    "\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "\n",
    "\n",
    "Совет: \n",
    "\n",
    "\n",
    "Что может помочь добиться лучшего результата (от простого)? \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "- можно поиграться [порогом](https://machinelearningmastery.com/threshold-moving-for-imbalanced-classification/)\n",
    "   \n",
    "\n",
    "\n",
    "\n",
    "- сгенерировать новые фичи, например  например посчитать число слов в тексте, длину слов итп итд. Или с помощью [тематического моделирования](https://pythobyte.com/python-for-nlp-topic-modeling-8fb3d689/) \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "- использование предбученной модели Берта, выбрав соответствующую модель и используя полученные эмбединги, даже на небольшом тренировочном датасете можно обучить модель, которая на test покажет хорошую метрику. В этом случаи можно сразу получить метрику > 0.95 (при правильно выбранной модели)\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\">\n",
    "<font size=\"5\"><b>Комментарий ревьюера</b></font>\n",
    "\n",
    "Ошибка:\n",
    "\n",
    "А вот тут, в самом конце,  выбрав лучшую модель на валидации, проверяем ее на тестовом датасете - делаем финальное тестирование. И если лучшая модель выбранная на валидационной покажет на test результат хуже требуемого, мы начнем процесс моделирования сначала (а не будем такие - \"а давай попробуем на тесте модель которая на валидации не была лучшей, может она нам на test даст нужное качество\").         \n",
    "    \n",
    "Почему только лучшая?! Это делается для того, чтобы мы даже незначительным образом не \"подгонялись\" под тестовую выборку. Ведь на train модели обучаются, по валидиации подгоняются гиперпараметры. Эти данные модели \"знают\". А test (out-of-sample) это уже моделирование прогноза на реальных данных и ситуации когда у нас есть уже лучшая модель (в рельности у нас же не может быть несоклько прогнозов, что то в любом случаи надо выбирать). Вот поэтому такая двухуровневая проверка на подгонку. Кроме того использование мноих моделей с разными гиперпараметрами это тоже подгонка, поэтому выбирая одну и тестируя только ее, мы тем самым боремся с подгонкой через использование многих-многих моделей, когда результат хорош не потому что мы данные почистили хорошо, моделировали правильно итд итп, а потому что из многих моделей хоть какая то случайно \"сыграет\". \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Выводы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Итак, в ходе работы, с целью найти инструмент, который будет искать токсичные комментарии и отправлять их на модерацию, была найден модель классификации комментариев на позитивные и негативные со значением метрики качества F1 >= 0.75.\n",
    "\n",
    "В том числе:\n",
    "\n",
    "- Подготовлены и проверены данные.\n",
    "- Очищены от лишних символов, проверены разметка.\n",
    "- Данные векторизованы методом TF-IDF.\n",
    "- Разделены на обучающую и тестовую выборки (Размер тренировочной выборки- 75%. Размер тестовой выборки - 25%).\n",
    "- С гиперпараметрами обучены две модели: LogisticRegression, LGBMClassifier. \n",
    "\n",
    "Наилучшей моделью стала LogisticRegression (при: C = 10) со значением F1 = 0.79. Ее можно рекомендовать к использованию. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<div class=\"alert alert-info\">\n",
    "<font size=\"5\"><b>Комментарий ревьюера</b></font>\n",
    "\n",
    "\n",
    "\n",
    "Аня, у тебя старательно выполненная работа, все четко, осмысленно. \n",
    "\n",
    "\n",
    "Замечания на будущее:\n",
    "    \n",
    "\n",
    "\n",
    "- Где то ты используешь GS, где то вручную прописаный цикл. Зачем? Используй везде GS\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Я оставил небольшие советы и вопросики (если есть время и желание можешь воспользоваться/ответить).\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "Обязательное к исправлению:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "- WordNetLemmatizer используем с POS - тег  и применяем к словам а не предложениям\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "- .astype('U') лишнее, стоит экономить ресурсы, иначе может даже ядро обрушиться\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "- если захочешь продолжить подбор гиперпараметров в вручную прописанном цикле, тогда тебе потребуется ещё валидационная выборка    \n",
    "\n",
    "\n",
    "- на test датасете тестируем только лучшую модель (нарушена логика использования датасетов при моделировании)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "Жду исправлений, для принятия проекта. Если какие то вопросы, то сразу спрашивай ) \n",
    "\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<div class=\"alert alert-info\">\n",
    "<font size=\"5\"><b>Комментарий ревьюераV2</b></font>\n",
    "\n",
    "\n",
    "Ещё раз извиняюсь за \"дура\" ) Исправил\n",
    "    \n",
    "    \n",
    "Спасибо за работу! Довел до ума pipeline, молодец. \n",
    "Красное исправлено. Надеюсь мои советы и вопросики были полезны и в копилочку знаний упало что то новое, а проект стал лучше, и симпатичней.\n",
    "\n",
    "\n",
    "  \n",
    "Отличная работа  Аня. Желаю успехов в дальнейшей учебе!\n",
    "\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Совет: \n",
    "    \n",
    "    \n",
    "Если думаешь и дальше заниматься NLP, то впереди очень современный и модный сейчас подход с использованием эмбедингов от Берт. Вообще именно в НЛП сейчас самые большие прорывы в машинном обучении,  может в курсе про [chatGPT](https://www.youtube.com/watch?v=IMP1zZ9K4Wc&t=3038s), GPT - это братик Берта )\n",
    "\n",
    "   \n",
    "\n",
    "<font color='green'><b>Полезные (и просто интересные) материалы:</b> \\\n",
    "Для работы с текстами используют и другие подходы. Например, сейчас активно используются RNN (LSTM) и трансформеры (BERT и другие с улицы Сезам, например, ELMO). НО! Они не являются панацеей, не всегда они нужны, так как и TF-IDF или Word2Vec + модели из классического ML тоже могут справляться. \\\n",
    "BERT тяжелый, существует много его вариаций для разных задач, есть готовые модели, есть надстройки над библиотекой transformers. Если, обучать BERT на GPU (можно в Google Colab или Kaggle), то должно быть побыстрее.\\\n",
    "https://huggingface.co/transformers/model_doc/bert.html \\\n",
    "https://t.me/renat_alimbekov \\\n",
    "https://colah.github.io/posts/2015-08-Understanding-LSTMs/ - Про LSTM \\\n",
    "https://web.stanford.edu/~jurafsky/slp3/10.pdf - про энкодер-декодер модели, этеншены\\\n",
    "https://pytorch.org/tutorials/beginner/transformer_tutorial.html - официальный гайд\n",
    "по трансформеру от создателей pytorch\\\n",
    "https://transformer.huggingface.co/ - поболтать с трансформером \\\n",
    "Библиотеки: allennlp, fairseq, transformers, tensorflow-text — множествореализованных\n",
    "методов для трансформеров методов NLP \\\n",
    "Word2Vec https://radimrehurek.com/gensim/models/word2vec.html \n",
    "\n",
    "\n",
    "Если понравилась работа с текстами, то можешь посмотреть очень интересный (но очень-очень сложный) курс лекций: https://github.com/yandexdataschool/nlp_course .\n",
    "\n",
    "Если нравится смотреть и слушать то есть целый курс на Ютубе https://www.youtube.com/watch?v=qDMwIQRQt-M&list=PLEwK9wdS5g0qksxWxtE5c2KuFkIfUXe3i&index=1\n",
    "\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Чек-лист проверки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [x]  Jupyter Notebook открыт\n",
    "- [x]  Весь код выполняется без ошибок\n",
    "- [x]  Ячейки с кодом расположены в порядке исполнения\n",
    "- [x]  Данные загружены и подготовлены\n",
    "- [x]  Модели обучены\n",
    "- [x]  Значение метрики *F1* не меньше 0.75\n",
    "- [x]  Выводы написаны"
   ]
  }
 ],
 "metadata": {
  "ExecuteTimeLog": [
   {
    "duration": 1932,
    "start_time": "2023-02-20T09:13:04.892Z"
   },
   {
    "duration": 4,
    "start_time": "2023-02-20T09:14:02.964Z"
   },
   {
    "duration": 177,
    "start_time": "2023-02-20T09:16:39.841Z"
   },
   {
    "duration": 27,
    "start_time": "2023-02-20T09:16:56.643Z"
   },
   {
    "duration": 2481,
    "start_time": "2023-02-20T09:17:33.730Z"
   },
   {
    "duration": 37,
    "start_time": "2023-02-20T09:17:37.252Z"
   },
   {
    "duration": 127,
    "start_time": "2023-02-20T09:20:11.856Z"
   },
   {
    "duration": 12,
    "start_time": "2023-02-20T09:20:33.576Z"
   },
   {
    "duration": 244,
    "start_time": "2023-02-20T09:24:53.844Z"
   },
   {
    "duration": 201,
    "start_time": "2023-02-20T09:25:43.884Z"
   },
   {
    "duration": 224,
    "start_time": "2023-02-20T09:25:52.109Z"
   },
   {
    "duration": 225,
    "start_time": "2023-02-20T09:26:17.070Z"
   },
   {
    "duration": 70,
    "start_time": "2023-02-20T09:27:22.214Z"
   },
   {
    "duration": 3,
    "start_time": "2023-02-20T09:31:50.157Z"
   },
   {
    "duration": 4222,
    "start_time": "2023-02-20T09:31:52.219Z"
   },
   {
    "duration": 3490,
    "start_time": "2023-02-20T09:32:23.051Z"
   },
   {
    "duration": 3,
    "start_time": "2023-02-20T09:33:22.021Z"
   },
   {
    "duration": 361,
    "start_time": "2023-02-20T09:33:40.869Z"
   },
   {
    "duration": 4,
    "start_time": "2023-02-20T09:35:48.375Z"
   },
   {
    "duration": 4,
    "start_time": "2023-02-20T09:35:52.423Z"
   },
   {
    "duration": 334,
    "start_time": "2023-02-20T09:35:55.159Z"
   },
   {
    "duration": 3,
    "start_time": "2023-02-20T09:36:36.799Z"
   },
   {
    "duration": 3,
    "start_time": "2023-02-20T09:36:38.392Z"
   },
   {
    "duration": 424,
    "start_time": "2023-02-20T09:36:40.528Z"
   },
   {
    "duration": 147,
    "start_time": "2023-02-20T09:37:25.873Z"
   },
   {
    "duration": 4,
    "start_time": "2023-02-20T09:37:27.752Z"
   },
   {
    "duration": 412,
    "start_time": "2023-02-20T09:37:30.201Z"
   },
   {
    "duration": 6124,
    "start_time": "2023-02-20T09:39:38.657Z"
   },
   {
    "duration": 8,
    "start_time": "2023-02-20T09:40:11.383Z"
   },
   {
    "duration": 4,
    "start_time": "2023-02-20T09:40:54.099Z"
   },
   {
    "duration": 1651,
    "start_time": "2023-02-20T09:40:59.683Z"
   },
   {
    "duration": 2370,
    "start_time": "2023-02-20T09:41:21.596Z"
   },
   {
    "duration": 2510,
    "start_time": "2023-02-20T09:42:00.998Z"
   },
   {
    "duration": 1502,
    "start_time": "2023-02-20T09:42:50.916Z"
   },
   {
    "duration": 920,
    "start_time": "2023-02-20T09:42:53.772Z"
   },
   {
    "duration": 41,
    "start_time": "2023-02-20T09:42:56.300Z"
   },
   {
    "duration": 272,
    "start_time": "2023-02-20T09:42:59.028Z"
   },
   {
    "duration": 12,
    "start_time": "2023-02-20T09:43:01.982Z"
   },
   {
    "duration": 7537,
    "start_time": "2023-02-20T09:43:04.877Z"
   },
   {
    "duration": 9,
    "start_time": "2023-02-20T09:43:21.445Z"
   },
   {
    "duration": 1669,
    "start_time": "2023-03-06T06:56:51.337Z"
   },
   {
    "duration": 2612,
    "start_time": "2023-03-06T06:56:53.471Z"
   },
   {
    "duration": 47,
    "start_time": "2023-03-06T06:56:57.880Z"
   },
   {
    "duration": 240,
    "start_time": "2023-03-06T06:57:00.232Z"
   },
   {
    "duration": 11,
    "start_time": "2023-03-06T06:57:02.345Z"
   },
   {
    "duration": 6855,
    "start_time": "2023-03-06T06:57:04.112Z"
   },
   {
    "duration": 8,
    "start_time": "2023-03-06T06:57:12.384Z"
   },
   {
    "duration": 46,
    "start_time": "2023-03-06T06:58:29.490Z"
   },
   {
    "duration": 3,
    "start_time": "2023-03-06T06:58:43.308Z"
   },
   {
    "duration": 113,
    "start_time": "2023-03-06T07:03:19.152Z"
   },
   {
    "duration": 71,
    "start_time": "2023-03-06T07:04:10.416Z"
   },
   {
    "duration": 57,
    "start_time": "2023-03-06T07:04:28.529Z"
   },
   {
    "duration": 11,
    "start_time": "2023-03-06T07:05:52.009Z"
   },
   {
    "duration": 63,
    "start_time": "2023-03-06T07:06:30.603Z"
   },
   {
    "duration": 97,
    "start_time": "2023-03-06T07:06:47.211Z"
   },
   {
    "duration": 1627,
    "start_time": "2023-03-23T05:32:59.142Z"
   },
   {
    "duration": 1244,
    "start_time": "2023-03-23T05:33:02.052Z"
   },
   {
    "duration": 41,
    "start_time": "2023-03-23T05:33:04.853Z"
   },
   {
    "duration": 266,
    "start_time": "2023-03-23T05:33:07.397Z"
   },
   {
    "duration": 10,
    "start_time": "2023-03-23T05:33:09.233Z"
   },
   {
    "duration": 6427,
    "start_time": "2023-03-23T05:33:12.948Z"
   },
   {
    "duration": 7,
    "start_time": "2023-03-23T05:33:19.835Z"
   },
   {
    "duration": 12,
    "start_time": "2023-03-23T05:34:52.525Z"
   },
   {
    "duration": 116,
    "start_time": "2023-03-23T05:37:01.929Z"
   },
   {
    "duration": 66,
    "start_time": "2023-03-23T05:37:23.134Z"
   },
   {
    "duration": 63,
    "start_time": "2023-03-23T05:39:28.787Z"
   },
   {
    "duration": 108,
    "start_time": "2023-03-23T05:48:01.831Z"
   },
   {
    "duration": 96,
    "start_time": "2023-03-23T05:48:22.729Z"
   },
   {
    "duration": 91,
    "start_time": "2023-03-23T05:49:09.181Z"
   },
   {
    "duration": 13,
    "start_time": "2023-03-23T05:50:29.549Z"
   },
   {
    "duration": 67,
    "start_time": "2023-03-23T05:50:59.834Z"
   },
   {
    "duration": 65,
    "start_time": "2023-03-23T05:51:05.048Z"
   },
   {
    "duration": 79,
    "start_time": "2023-03-23T05:51:32.217Z"
   },
   {
    "duration": 87,
    "start_time": "2023-03-23T05:51:56.784Z"
   },
   {
    "duration": 1593,
    "start_time": "2023-03-23T05:53:25.586Z"
   },
   {
    "duration": 11,
    "start_time": "2023-03-23T05:54:52.723Z"
   },
   {
    "duration": 532,
    "start_time": "2023-03-23T05:56:06.727Z"
   },
   {
    "duration": 171,
    "start_time": "2023-03-23T05:59:07.449Z"
   },
   {
    "duration": 5,
    "start_time": "2023-03-23T06:01:03.442Z"
   },
   {
    "duration": 4,
    "start_time": "2023-03-23T06:01:32.186Z"
   },
   {
    "duration": 6198,
    "start_time": "2023-03-23T06:03:59.389Z"
   },
   {
    "duration": 5903,
    "start_time": "2023-03-23T06:04:38.334Z"
   },
   {
    "duration": 2165,
    "start_time": "2023-03-23T06:04:55.342Z"
   },
   {
    "duration": 13,
    "start_time": "2023-03-23T07:11:07.355Z"
   },
   {
    "duration": 6,
    "start_time": "2023-03-23T07:12:08.938Z"
   },
   {
    "duration": 851,
    "start_time": "2023-03-23T07:12:10.483Z"
   },
   {
    "duration": 14,
    "start_time": "2023-03-23T07:13:42.244Z"
   },
   {
    "duration": 13,
    "start_time": "2023-03-23T07:15:00.287Z"
   },
   {
    "duration": 3,
    "start_time": "2023-03-23T07:22:24.407Z"
   },
   {
    "duration": 713,
    "start_time": "2023-03-23T07:22:28.128Z"
   },
   {
    "duration": 12,
    "start_time": "2023-03-23T07:24:00.125Z"
   },
   {
    "duration": 38,
    "start_time": "2023-03-23T07:25:46.059Z"
   },
   {
    "duration": 415,
    "start_time": "2023-03-23T07:25:48.430Z"
   },
   {
    "duration": 43855,
    "start_time": "2023-03-23T07:26:58.431Z"
   },
   {
    "duration": 10921,
    "start_time": "2023-03-23T07:30:34.623Z"
   },
   {
    "duration": 11639,
    "start_time": "2023-03-23T07:31:29.376Z"
   },
   {
    "duration": 3,
    "start_time": "2023-03-23T07:32:23.875Z"
   },
   {
    "duration": 12440,
    "start_time": "2023-03-23T07:32:25.690Z"
   },
   {
    "duration": 44561,
    "start_time": "2023-03-23T07:43:58.850Z"
   },
   {
    "duration": 297,
    "start_time": "2023-03-23T08:20:23.866Z"
   },
   {
    "duration": 66385,
    "start_time": "2023-03-23T08:43:13.048Z"
   },
   {
    "duration": 18,
    "start_time": "2023-03-23T08:44:59.730Z"
   },
   {
    "duration": 67,
    "start_time": "2023-03-23T08:45:25.451Z"
   },
   {
    "duration": 32,
    "start_time": "2023-03-23T08:55:40.702Z"
   },
   {
    "duration": 25,
    "start_time": "2023-03-23T08:56:03.398Z"
   },
   {
    "duration": 13,
    "start_time": "2023-03-23T08:57:12.503Z"
   },
   {
    "duration": 4,
    "start_time": "2023-03-23T08:57:45.714Z"
   },
   {
    "duration": 392803,
    "start_time": "2023-03-23T08:57:48.935Z"
   },
   {
    "duration": 489,
    "start_time": "2023-03-23T09:08:43.171Z"
   },
   {
    "duration": 1442,
    "start_time": "2023-03-27T06:05:10.832Z"
   },
   {
    "duration": 3870,
    "start_time": "2023-03-27T06:05:12.983Z"
   },
   {
    "duration": 42,
    "start_time": "2023-03-27T06:05:18.287Z"
   },
   {
    "duration": 223,
    "start_time": "2023-03-27T06:05:21.216Z"
   },
   {
    "duration": 10,
    "start_time": "2023-03-27T06:07:53.970Z"
   },
   {
    "duration": 6102,
    "start_time": "2023-03-27T06:08:10.242Z"
   },
   {
    "duration": 7,
    "start_time": "2023-03-27T06:08:37.099Z"
   },
   {
    "duration": 9,
    "start_time": "2023-03-27T06:08:41.795Z"
   },
   {
    "duration": 52,
    "start_time": "2023-03-27T06:08:50.154Z"
   },
   {
    "duration": 6,
    "start_time": "2023-03-27T06:10:10.852Z"
   },
   {
    "duration": 114,
    "start_time": "2023-03-27T06:11:07.422Z"
   },
   {
    "duration": 97,
    "start_time": "2023-03-27T06:11:40.244Z"
   },
   {
    "duration": 95,
    "start_time": "2023-03-27T06:11:47.911Z"
   },
   {
    "duration": 103,
    "start_time": "2023-03-27T06:12:04.998Z"
   },
   {
    "duration": 156,
    "start_time": "2023-03-27T06:12:12.991Z"
   },
   {
    "duration": 99,
    "start_time": "2023-03-27T06:12:18.375Z"
   },
   {
    "duration": 104,
    "start_time": "2023-03-27T06:12:23.966Z"
   },
   {
    "duration": 104,
    "start_time": "2023-03-27T06:12:40.151Z"
   },
   {
    "duration": 99,
    "start_time": "2023-03-27T06:12:45.720Z"
   },
   {
    "duration": 100,
    "start_time": "2023-03-27T06:13:04.264Z"
   },
   {
    "duration": 276,
    "start_time": "2023-03-27T06:14:02.538Z"
   },
   {
    "duration": 120,
    "start_time": "2023-03-27T06:14:10.077Z"
   },
   {
    "duration": 106,
    "start_time": "2023-03-27T06:14:44.681Z"
   },
   {
    "duration": 102,
    "start_time": "2023-03-27T06:15:10.986Z"
   },
   {
    "duration": 103,
    "start_time": "2023-03-27T06:15:32.708Z"
   },
   {
    "duration": 6,
    "start_time": "2023-03-27T06:16:15.659Z"
   },
   {
    "duration": 4,
    "start_time": "2023-03-27T06:17:10.076Z"
   },
   {
    "duration": 5,
    "start_time": "2023-03-27T06:17:47.829Z"
   },
   {
    "duration": 117,
    "start_time": "2023-03-27T06:18:27.518Z"
   },
   {
    "duration": 102,
    "start_time": "2023-03-27T06:18:46.750Z"
   },
   {
    "duration": 12,
    "start_time": "2023-03-27T06:20:49.639Z"
   },
   {
    "duration": 51,
    "start_time": "2023-03-27T06:20:52.895Z"
   },
   {
    "duration": 5,
    "start_time": "2023-03-27T06:20:54.824Z"
   },
   {
    "duration": 100,
    "start_time": "2023-03-27T06:21:06.263Z"
   },
   {
    "duration": 113,
    "start_time": "2023-03-27T06:21:19.408Z"
   },
   {
    "duration": 1353,
    "start_time": "2023-03-27T06:22:51.498Z"
   },
   {
    "duration": 52,
    "start_time": "2023-03-27T06:23:04.273Z"
   },
   {
    "duration": 1672,
    "start_time": "2023-03-27T06:23:22.514Z"
   },
   {
    "duration": 4106,
    "start_time": "2023-03-27T06:23:26.441Z"
   },
   {
    "duration": 40,
    "start_time": "2023-03-27T06:23:31.875Z"
   },
   {
    "duration": 261,
    "start_time": "2023-03-27T06:23:34.450Z"
   },
   {
    "duration": 10,
    "start_time": "2023-03-27T06:23:36.537Z"
   },
   {
    "duration": 6404,
    "start_time": "2023-03-27T06:23:38.697Z"
   },
   {
    "duration": 7,
    "start_time": "2023-03-27T06:23:46.578Z"
   },
   {
    "duration": 21,
    "start_time": "2023-03-27T06:23:50.466Z"
   },
   {
    "duration": 86,
    "start_time": "2023-03-27T06:23:54.817Z"
   },
   {
    "duration": 6,
    "start_time": "2023-03-27T06:23:56.690Z"
   },
   {
    "duration": 134,
    "start_time": "2023-03-27T06:23:58.362Z"
   },
   {
    "duration": 56,
    "start_time": "2023-03-27T06:24:00.907Z"
   },
   {
    "duration": 4,
    "start_time": "2023-03-27T06:24:03.411Z"
   },
   {
    "duration": 127,
    "start_time": "2023-03-27T06:24:05.034Z"
   },
   {
    "duration": 1318,
    "start_time": "2023-03-27T06:24:07.562Z"
   },
   {
    "duration": 56,
    "start_time": "2023-03-27T06:24:27.651Z"
   },
   {
    "duration": 1441,
    "start_time": "2023-03-27T06:24:53.997Z"
   },
   {
    "duration": 4046,
    "start_time": "2023-03-27T06:24:56.763Z"
   },
   {
    "duration": 38,
    "start_time": "2023-03-27T06:25:02.979Z"
   },
   {
    "duration": 246,
    "start_time": "2023-03-27T06:25:05.251Z"
   },
   {
    "duration": 9,
    "start_time": "2023-03-27T06:25:07.611Z"
   },
   {
    "duration": 6729,
    "start_time": "2023-03-27T06:25:09.627Z"
   },
   {
    "duration": 8,
    "start_time": "2023-03-27T06:25:17.756Z"
   },
   {
    "duration": 10,
    "start_time": "2023-03-27T06:25:19.955Z"
   },
   {
    "duration": 80,
    "start_time": "2023-03-27T06:25:21.755Z"
   },
   {
    "duration": 7,
    "start_time": "2023-03-27T06:25:24.133Z"
   },
   {
    "duration": 151,
    "start_time": "2023-03-27T06:25:25.667Z"
   },
   {
    "duration": 69,
    "start_time": "2023-03-27T06:25:28.547Z"
   },
   {
    "duration": 5,
    "start_time": "2023-03-27T06:25:30.796Z"
   },
   {
    "duration": 108,
    "start_time": "2023-03-27T06:25:32.292Z"
   },
   {
    "duration": 1288,
    "start_time": "2023-03-27T06:25:34.619Z"
   },
   {
    "duration": 43,
    "start_time": "2023-03-27T06:25:54.196Z"
   },
   {
    "duration": 1382,
    "start_time": "2023-03-27T06:26:25.108Z"
   },
   {
    "duration": 3947,
    "start_time": "2023-03-27T06:26:27.412Z"
   },
   {
    "duration": 42,
    "start_time": "2023-03-27T06:26:32.948Z"
   },
   {
    "duration": 240,
    "start_time": "2023-03-27T06:26:35.175Z"
   },
   {
    "duration": 16,
    "start_time": "2023-03-27T06:26:37.165Z"
   },
   {
    "duration": 6673,
    "start_time": "2023-03-27T06:26:39.339Z"
   },
   {
    "duration": 14,
    "start_time": "2023-03-27T06:26:46.013Z"
   },
   {
    "duration": 61,
    "start_time": "2023-03-27T06:26:48.774Z"
   },
   {
    "duration": 9,
    "start_time": "2023-03-27T06:26:50.669Z"
   },
   {
    "duration": 117,
    "start_time": "2023-03-27T06:26:52.687Z"
   },
   {
    "duration": 54,
    "start_time": "2023-03-27T06:26:54.885Z"
   },
   {
    "duration": 6,
    "start_time": "2023-03-27T06:26:56.964Z"
   },
   {
    "duration": 192,
    "start_time": "2023-03-27T06:26:59.029Z"
   },
   {
    "duration": 1297,
    "start_time": "2023-03-27T06:27:01.997Z"
   },
   {
    "duration": 211,
    "start_time": "2023-03-27T06:27:13.300Z"
   },
   {
    "duration": 5859,
    "start_time": "2023-03-27T06:27:22.017Z"
   },
   {
    "duration": 128,
    "start_time": "2023-03-27T06:27:29.748Z"
   },
   {
    "duration": 37,
    "start_time": "2023-03-27T06:28:13.502Z"
   },
   {
    "duration": 56,
    "start_time": "2023-03-27T06:28:16.127Z"
   },
   {
    "duration": 1368,
    "start_time": "2023-03-27T06:28:55.576Z"
   },
   {
    "duration": 3793,
    "start_time": "2023-03-27T06:28:58.351Z"
   },
   {
    "duration": 38,
    "start_time": "2023-03-27T06:29:04.943Z"
   },
   {
    "duration": 228,
    "start_time": "2023-03-27T06:29:07.711Z"
   },
   {
    "duration": 10,
    "start_time": "2023-03-27T06:29:10.040Z"
   },
   {
    "duration": 6184,
    "start_time": "2023-03-27T06:29:12.184Z"
   },
   {
    "duration": 7,
    "start_time": "2023-03-27T06:29:20.688Z"
   },
   {
    "duration": 11,
    "start_time": "2023-03-27T06:29:22.983Z"
   },
   {
    "duration": 55,
    "start_time": "2023-03-27T06:29:27.776Z"
   },
   {
    "duration": 4,
    "start_time": "2023-03-27T06:30:15.818Z"
   },
   {
    "duration": 3,
    "start_time": "2023-03-27T06:30:18.946Z"
   },
   {
    "duration": 3,
    "start_time": "2023-03-27T06:31:14.619Z"
   },
   {
    "duration": 4,
    "start_time": "2023-03-27T06:34:55.725Z"
   },
   {
    "duration": 6,
    "start_time": "2023-03-27T06:35:11.086Z"
   },
   {
    "duration": 115,
    "start_time": "2023-03-27T06:35:12.830Z"
   },
   {
    "duration": 57,
    "start_time": "2023-03-27T06:35:15.952Z"
   },
   {
    "duration": 6,
    "start_time": "2023-03-27T06:35:18.414Z"
   },
   {
    "duration": 110,
    "start_time": "2023-03-27T06:35:21.494Z"
   },
   {
    "duration": 1205,
    "start_time": "2023-03-27T06:35:25.014Z"
   },
   {
    "duration": 216,
    "start_time": "2023-03-27T06:35:59.398Z"
   },
   {
    "duration": 5,
    "start_time": "2023-03-27T06:36:12.399Z"
   },
   {
    "duration": 1372,
    "start_time": "2023-03-27T06:38:28.577Z"
   },
   {
    "duration": 3807,
    "start_time": "2023-03-27T06:38:31.032Z"
   },
   {
    "duration": 37,
    "start_time": "2023-03-27T06:38:36.177Z"
   },
   {
    "duration": 229,
    "start_time": "2023-03-27T06:38:38.960Z"
   },
   {
    "duration": 10,
    "start_time": "2023-03-27T06:38:40.889Z"
   },
   {
    "duration": 5868,
    "start_time": "2023-03-27T06:38:43.080Z"
   },
   {
    "duration": 6,
    "start_time": "2023-03-27T06:38:50.353Z"
   },
   {
    "duration": 9,
    "start_time": "2023-03-27T06:38:52.744Z"
   },
   {
    "duration": 51,
    "start_time": "2023-03-27T06:38:54.904Z"
   },
   {
    "duration": 4,
    "start_time": "2023-03-27T06:38:56.769Z"
   },
   {
    "duration": 4,
    "start_time": "2023-03-27T06:38:59.553Z"
   },
   {
    "duration": 6,
    "start_time": "2023-03-27T06:39:03.904Z"
   },
   {
    "duration": 48,
    "start_time": "2023-03-27T06:39:08.025Z"
   },
   {
    "duration": 4,
    "start_time": "2023-03-27T06:39:15.049Z"
   },
   {
    "duration": 214,
    "start_time": "2023-03-27T06:39:21.153Z"
   },
   {
    "duration": 1233,
    "start_time": "2023-03-27T06:39:24.169Z"
   },
   {
    "duration": 107,
    "start_time": "2023-03-27T06:39:45.969Z"
   },
   {
    "duration": 5035,
    "start_time": "2023-03-27T06:40:11.178Z"
   },
   {
    "duration": 41,
    "start_time": "2023-03-27T06:42:25.797Z"
   },
   {
    "duration": 1433,
    "start_time": "2023-03-27T06:42:52.148Z"
   },
   {
    "duration": 3973,
    "start_time": "2023-03-27T06:42:54.925Z"
   },
   {
    "duration": 35,
    "start_time": "2023-03-27T06:42:59.709Z"
   },
   {
    "duration": 224,
    "start_time": "2023-03-27T06:43:01.988Z"
   },
   {
    "duration": 11,
    "start_time": "2023-03-27T06:43:03.948Z"
   },
   {
    "duration": 6319,
    "start_time": "2023-03-27T06:43:06.229Z"
   },
   {
    "duration": 7,
    "start_time": "2023-03-27T06:43:14.796Z"
   },
   {
    "duration": 10,
    "start_time": "2023-03-27T06:43:52.734Z"
   },
   {
    "duration": 52,
    "start_time": "2023-03-27T06:43:55.086Z"
   },
   {
    "duration": 4,
    "start_time": "2023-03-27T06:43:57.126Z"
   },
   {
    "duration": 6,
    "start_time": "2023-03-27T06:43:59.583Z"
   },
   {
    "duration": 122,
    "start_time": "2023-03-27T06:44:01.190Z"
   },
   {
    "duration": 54,
    "start_time": "2023-03-27T06:44:03.510Z"
   },
   {
    "duration": 4,
    "start_time": "2023-03-27T06:44:06.283Z"
   },
   {
    "duration": 112,
    "start_time": "2023-03-27T06:44:08.246Z"
   },
   {
    "duration": 145,
    "start_time": "2023-03-27T06:44:11.694Z"
   },
   {
    "duration": 39,
    "start_time": "2023-03-27T06:48:47.138Z"
   },
   {
    "duration": 115,
    "start_time": "2023-03-27T06:48:48.890Z"
   },
   {
    "duration": 1355,
    "start_time": "2023-03-27T06:58:53.294Z"
   },
   {
    "duration": 723,
    "start_time": "2023-03-27T06:58:55.902Z"
   },
   {
    "duration": 40,
    "start_time": "2023-03-27T06:58:57.958Z"
   },
   {
    "duration": 226,
    "start_time": "2023-03-27T06:59:00.527Z"
   },
   {
    "duration": 10,
    "start_time": "2023-03-27T06:59:02.854Z"
   },
   {
    "duration": 6150,
    "start_time": "2023-03-27T06:59:04.630Z"
   },
   {
    "duration": 11,
    "start_time": "2023-03-27T06:59:12.382Z"
   },
   {
    "duration": 9,
    "start_time": "2023-03-27T06:59:15.039Z"
   },
   {
    "duration": 113,
    "start_time": "2023-03-27T06:59:16.846Z"
   },
   {
    "duration": 64,
    "start_time": "2023-03-27T06:59:20.054Z"
   },
   {
    "duration": 4,
    "start_time": "2023-03-27T06:59:22.262Z"
   },
   {
    "duration": 4,
    "start_time": "2023-03-27T06:59:24.399Z"
   },
   {
    "duration": 6,
    "start_time": "2023-03-27T06:59:26.926Z"
   },
   {
    "duration": 123,
    "start_time": "2023-03-27T06:59:28.518Z"
   },
   {
    "duration": 49,
    "start_time": "2023-03-27T06:59:31.519Z"
   },
   {
    "duration": 5,
    "start_time": "2023-03-27T06:59:34.015Z"
   },
   {
    "duration": 105,
    "start_time": "2023-03-27T06:59:35.679Z"
   },
   {
    "duration": 168,
    "start_time": "2023-03-27T06:59:38.887Z"
   },
   {
    "duration": 10,
    "start_time": "2023-03-27T06:59:44.380Z"
   },
   {
    "duration": 75,
    "start_time": "2023-03-27T07:02:11.818Z"
   },
   {
    "duration": 4,
    "start_time": "2023-03-27T07:03:08.283Z"
   },
   {
    "duration": 1846,
    "start_time": "2023-03-27T07:03:10.003Z"
   },
   {
    "duration": 1782,
    "start_time": "2023-03-27T07:03:42.986Z"
   },
   {
    "duration": 1940,
    "start_time": "2023-03-27T07:04:01.579Z"
   },
   {
    "duration": 32,
    "start_time": "2023-03-27T07:06:54.032Z"
   },
   {
    "duration": 42,
    "start_time": "2023-03-27T07:09:45.922Z"
   },
   {
    "duration": 35,
    "start_time": "2023-03-27T07:10:12.602Z"
   },
   {
    "duration": 17,
    "start_time": "2023-03-27T07:11:39.965Z"
   },
   {
    "duration": 11,
    "start_time": "2023-03-27T07:12:23.829Z"
   },
   {
    "duration": 1427,
    "start_time": "2023-03-27T07:12:39.757Z"
   },
   {
    "duration": 1316,
    "start_time": "2023-03-27T07:17:25.722Z"
   },
   {
    "duration": 751,
    "start_time": "2023-03-27T07:17:28.274Z"
   },
   {
    "duration": 38,
    "start_time": "2023-03-27T07:17:30.609Z"
   },
   {
    "duration": 217,
    "start_time": "2023-03-27T07:17:32.962Z"
   },
   {
    "duration": 11,
    "start_time": "2023-03-27T07:17:35.122Z"
   },
   {
    "duration": 5918,
    "start_time": "2023-03-27T07:17:37.155Z"
   },
   {
    "duration": 6,
    "start_time": "2023-03-27T07:17:43.075Z"
   },
   {
    "duration": 9,
    "start_time": "2023-03-27T07:17:45.035Z"
   },
   {
    "duration": 50,
    "start_time": "2023-03-27T07:17:47.298Z"
   },
   {
    "duration": 3,
    "start_time": "2023-03-27T07:17:49.163Z"
   },
   {
    "duration": 4,
    "start_time": "2023-03-27T07:17:51.410Z"
   },
   {
    "duration": 6,
    "start_time": "2023-03-27T07:17:53.242Z"
   },
   {
    "duration": 122,
    "start_time": "2023-03-27T07:17:55.403Z"
   },
   {
    "duration": 53,
    "start_time": "2023-03-27T07:17:57.603Z"
   },
   {
    "duration": 5,
    "start_time": "2023-03-27T07:17:59.466Z"
   },
   {
    "duration": 103,
    "start_time": "2023-03-27T07:18:01.491Z"
   },
   {
    "duration": 153,
    "start_time": "2023-03-27T07:18:58.219Z"
   },
   {
    "duration": 1235,
    "start_time": "2023-03-27T07:19:04.699Z"
   },
   {
    "duration": 157,
    "start_time": "2023-03-27T07:19:13.764Z"
   },
   {
    "duration": 4,
    "start_time": "2023-03-27T07:19:34.020Z"
   },
   {
    "duration": 1475,
    "start_time": "2023-03-27T07:20:29.534Z"
   },
   {
    "duration": 3788,
    "start_time": "2023-03-27T07:20:32.077Z"
   },
   {
    "duration": 37,
    "start_time": "2023-03-27T07:20:36.461Z"
   },
   {
    "duration": 236,
    "start_time": "2023-03-27T07:20:38.941Z"
   },
   {
    "duration": 11,
    "start_time": "2023-03-27T07:20:40.949Z"
   },
   {
    "duration": 5970,
    "start_time": "2023-03-27T07:20:42.901Z"
   },
   {
    "duration": 7,
    "start_time": "2023-03-27T07:20:48.873Z"
   },
   {
    "duration": 11,
    "start_time": "2023-03-27T07:20:51.317Z"
   },
   {
    "duration": 54,
    "start_time": "2023-03-27T07:20:53.933Z"
   },
   {
    "duration": 3,
    "start_time": "2023-03-27T07:20:55.813Z"
   },
   {
    "duration": 4,
    "start_time": "2023-03-27T07:20:58.014Z"
   },
   {
    "duration": 6,
    "start_time": "2023-03-27T07:21:00.094Z"
   },
   {
    "duration": 117,
    "start_time": "2023-03-27T07:21:01.879Z"
   },
   {
    "duration": 52,
    "start_time": "2023-03-27T07:21:04.166Z"
   },
   {
    "duration": 5,
    "start_time": "2023-03-27T07:21:07.221Z"
   },
   {
    "duration": 102,
    "start_time": "2023-03-27T07:21:08.830Z"
   },
   {
    "duration": 226,
    "start_time": "2023-03-27T07:21:33.438Z"
   },
   {
    "duration": 107,
    "start_time": "2023-03-27T07:21:52.903Z"
   },
   {
    "duration": 1214,
    "start_time": "2023-03-27T07:22:44.231Z"
   },
   {
    "duration": 4932,
    "start_time": "2023-03-27T07:22:56.823Z"
   },
   {
    "duration": 45,
    "start_time": "2023-03-27T07:23:39.017Z"
   },
   {
    "duration": 1363,
    "start_time": "2023-03-27T07:24:48.786Z"
   },
   {
    "duration": 3739,
    "start_time": "2023-03-27T07:24:51.449Z"
   },
   {
    "duration": 35,
    "start_time": "2023-03-27T07:24:57.283Z"
   },
   {
    "duration": 230,
    "start_time": "2023-03-27T07:24:59.754Z"
   },
   {
    "duration": 10,
    "start_time": "2023-03-27T07:25:01.994Z"
   },
   {
    "duration": 5819,
    "start_time": "2023-03-27T07:25:04.249Z"
   },
   {
    "duration": 1332,
    "start_time": "2023-03-27T07:26:28.831Z"
   },
   {
    "duration": 718,
    "start_time": "2023-03-27T07:26:32.747Z"
   },
   {
    "duration": 35,
    "start_time": "2023-03-27T07:26:34.708Z"
   },
   {
    "duration": 231,
    "start_time": "2023-03-27T07:26:37.123Z"
   },
   {
    "duration": 10,
    "start_time": "2023-03-27T07:26:39.499Z"
   },
   {
    "duration": 6017,
    "start_time": "2023-03-27T07:26:41.316Z"
   },
   {
    "duration": 6,
    "start_time": "2023-03-27T07:26:48.860Z"
   },
   {
    "duration": 9,
    "start_time": "2023-03-27T07:26:56.619Z"
   },
   {
    "duration": 54,
    "start_time": "2023-03-27T07:26:58.979Z"
   },
   {
    "duration": 4,
    "start_time": "2023-03-27T07:27:01.571Z"
   },
   {
    "duration": 5,
    "start_time": "2023-03-27T07:27:13.076Z"
   },
   {
    "duration": 113,
    "start_time": "2023-03-27T07:27:14.668Z"
   },
   {
    "duration": 56,
    "start_time": "2023-03-27T07:27:16.852Z"
   },
   {
    "duration": 5,
    "start_time": "2023-03-27T07:27:19.283Z"
   },
   {
    "duration": 114,
    "start_time": "2023-03-27T07:27:20.973Z"
   },
   {
    "duration": 220,
    "start_time": "2023-03-27T07:27:35.748Z"
   },
   {
    "duration": 1207,
    "start_time": "2023-03-27T07:28:07.940Z"
   },
   {
    "duration": 5125,
    "start_time": "2023-03-27T07:28:13.884Z"
   },
   {
    "duration": 40,
    "start_time": "2023-03-27T07:29:01.358Z"
   },
   {
    "duration": 5,
    "start_time": "2023-03-27T07:29:03.677Z"
   },
   {
    "duration": 1350,
    "start_time": "2023-03-27T07:29:18.910Z"
   },
   {
    "duration": 3768,
    "start_time": "2023-03-27T07:29:21.573Z"
   },
   {
    "duration": 34,
    "start_time": "2023-03-27T07:29:25.343Z"
   },
   {
    "duration": 220,
    "start_time": "2023-03-27T07:29:26.501Z"
   },
   {
    "duration": 10,
    "start_time": "2023-03-27T07:29:29.325Z"
   },
   {
    "duration": 6045,
    "start_time": "2023-03-27T07:29:31.342Z"
   },
   {
    "duration": 7,
    "start_time": "2023-03-27T07:29:37.389Z"
   },
   {
    "duration": 9,
    "start_time": "2023-03-27T07:29:39.606Z"
   },
   {
    "duration": 51,
    "start_time": "2023-03-27T07:29:42.030Z"
   },
   {
    "duration": 3,
    "start_time": "2023-03-27T07:29:43.797Z"
   },
   {
    "duration": 4,
    "start_time": "2023-03-27T07:29:45.476Z"
   },
   {
    "duration": 6,
    "start_time": "2023-03-27T07:29:47.700Z"
   },
   {
    "duration": 110,
    "start_time": "2023-03-27T07:29:49.439Z"
   },
   {
    "duration": 53,
    "start_time": "2023-03-27T07:29:51.873Z"
   },
   {
    "duration": 4,
    "start_time": "2023-03-27T07:29:54.096Z"
   },
   {
    "duration": 103,
    "start_time": "2023-03-27T07:29:56.527Z"
   },
   {
    "duration": 231,
    "start_time": "2023-03-27T07:30:00.098Z"
   },
   {
    "duration": 1219,
    "start_time": "2023-03-27T07:30:02.761Z"
   },
   {
    "duration": 4843,
    "start_time": "2023-03-27T07:30:05.543Z"
   },
   {
    "duration": 117,
    "start_time": "2023-03-27T07:31:18.739Z"
   },
   {
    "duration": 4,
    "start_time": "2023-03-27T07:31:33.224Z"
   },
   {
    "duration": 11,
    "start_time": "2023-03-27T07:31:35.075Z"
   },
   {
    "duration": 1432,
    "start_time": "2023-03-27T07:32:59.738Z"
   },
   {
    "duration": 3758,
    "start_time": "2023-03-27T07:33:02.410Z"
   },
   {
    "duration": 35,
    "start_time": "2023-03-27T07:33:06.865Z"
   },
   {
    "duration": 215,
    "start_time": "2023-03-27T07:33:09.046Z"
   },
   {
    "duration": 9,
    "start_time": "2023-03-27T07:33:11.250Z"
   },
   {
    "duration": 6195,
    "start_time": "2023-03-27T07:33:13.110Z"
   },
   {
    "duration": 6,
    "start_time": "2023-03-27T07:33:20.611Z"
   },
   {
    "duration": 11,
    "start_time": "2023-03-27T07:33:22.834Z"
   },
   {
    "duration": 51,
    "start_time": "2023-03-27T07:33:25.210Z"
   },
   {
    "duration": 3,
    "start_time": "2023-03-27T07:33:26.826Z"
   },
   {
    "duration": 4,
    "start_time": "2023-03-27T07:33:28.514Z"
   },
   {
    "duration": 7,
    "start_time": "2023-03-27T07:33:30.746Z"
   },
   {
    "duration": 112,
    "start_time": "2023-03-27T07:33:32.683Z"
   },
   {
    "duration": 48,
    "start_time": "2023-03-27T07:33:34.806Z"
   },
   {
    "duration": 6,
    "start_time": "2023-03-27T07:33:37.226Z"
   },
   {
    "duration": 104,
    "start_time": "2023-03-27T07:33:38.746Z"
   },
   {
    "duration": 7331,
    "start_time": "2023-03-27T07:33:43.643Z"
   },
   {
    "duration": 32,
    "start_time": "2023-03-27T07:34:00.555Z"
   },
   {
    "duration": 14,
    "start_time": "2023-03-27T07:34:20.332Z"
   },
   {
    "duration": 724,
    "start_time": "2023-03-27T07:36:11.630Z"
   },
   {
    "duration": 1390,
    "start_time": "2023-03-27T07:36:23.070Z"
   },
   {
    "duration": 754,
    "start_time": "2023-03-27T07:36:25.365Z"
   },
   {
    "duration": 39,
    "start_time": "2023-03-27T07:36:27.933Z"
   },
   {
    "duration": 235,
    "start_time": "2023-03-27T07:36:30.734Z"
   },
   {
    "duration": 10,
    "start_time": "2023-03-27T07:36:32.613Z"
   },
   {
    "duration": 6116,
    "start_time": "2023-03-27T07:36:35.402Z"
   },
   {
    "duration": 7,
    "start_time": "2023-03-27T07:36:41.520Z"
   },
   {
    "duration": 14,
    "start_time": "2023-03-27T07:36:52.126Z"
   },
   {
    "duration": 17,
    "start_time": "2023-03-27T07:37:12.607Z"
   },
   {
    "duration": 71,
    "start_time": "2023-03-27T07:37:14.518Z"
   },
   {
    "duration": 9662,
    "start_time": "2023-03-27T07:37:38.439Z"
   },
   {
    "duration": 5,
    "start_time": "2023-03-27T07:38:04.614Z"
   },
   {
    "duration": 6,
    "start_time": "2023-03-27T07:38:07.271Z"
   },
   {
    "duration": 116,
    "start_time": "2023-03-27T07:38:09.335Z"
   },
   {
    "duration": 134,
    "start_time": "2023-03-27T07:38:11.640Z"
   },
   {
    "duration": 35,
    "start_time": "2023-03-27T07:39:43.401Z"
   },
   {
    "duration": 10,
    "start_time": "2023-03-27T07:41:07.850Z"
   },
   {
    "duration": 37,
    "start_time": "2023-03-27T07:42:13.702Z"
   },
   {
    "duration": 1355,
    "start_time": "2023-03-27T07:43:10.453Z"
   },
   {
    "duration": 721,
    "start_time": "2023-03-27T07:43:12.837Z"
   },
   {
    "duration": 37,
    "start_time": "2023-03-27T07:43:15.844Z"
   },
   {
    "duration": 223,
    "start_time": "2023-03-27T07:43:18.309Z"
   },
   {
    "duration": 10,
    "start_time": "2023-03-27T07:43:20.774Z"
   },
   {
    "duration": 6016,
    "start_time": "2023-03-27T07:43:22.813Z"
   },
   {
    "duration": 8,
    "start_time": "2023-03-27T07:43:30.158Z"
   },
   {
    "duration": 15,
    "start_time": "2023-03-27T07:43:32.541Z"
   },
   {
    "duration": 53,
    "start_time": "2023-03-27T07:43:35.629Z"
   },
   {
    "duration": 3,
    "start_time": "2023-03-27T07:43:37.797Z"
   },
   {
    "duration": 4,
    "start_time": "2023-03-27T07:43:40.237Z"
   },
   {
    "duration": 6,
    "start_time": "2023-03-27T07:43:42.606Z"
   },
   {
    "duration": 119,
    "start_time": "2023-03-27T07:43:44.262Z"
   },
   {
    "duration": 55,
    "start_time": "2023-03-27T07:43:47.102Z"
   },
   {
    "duration": 5,
    "start_time": "2023-03-27T07:43:53.045Z"
   },
   {
    "duration": 105,
    "start_time": "2023-03-27T07:43:55.343Z"
   },
   {
    "duration": 9703,
    "start_time": "2023-03-27T07:43:58.395Z"
   },
   {
    "duration": 134,
    "start_time": "2023-03-27T07:47:13.595Z"
   },
   {
    "duration": 558,
    "start_time": "2023-03-27T07:48:53.195Z"
   },
   {
    "duration": 14,
    "start_time": "2023-03-27T07:50:15.902Z"
   },
   {
    "duration": 10,
    "start_time": "2023-03-27T07:54:32.257Z"
   },
   {
    "duration": 10,
    "start_time": "2023-03-27T07:55:34.002Z"
   },
   {
    "duration": 52415,
    "start_time": "2023-03-27T07:55:52.274Z"
   },
   {
    "duration": 10,
    "start_time": "2023-03-27T07:57:31.748Z"
   },
   {
    "duration": 137,
    "start_time": "2023-03-27T07:58:15.221Z"
   },
   {
    "duration": 111,
    "start_time": "2023-03-27T07:58:29.317Z"
   },
   {
    "duration": 108,
    "start_time": "2023-03-27T08:00:27.607Z"
   },
   {
    "duration": 108,
    "start_time": "2023-03-27T08:00:42.839Z"
   },
   {
    "duration": 9940,
    "start_time": "2023-03-27T08:01:07.360Z"
   },
   {
    "duration": 3,
    "start_time": "2023-03-27T08:01:32.111Z"
   },
   {
    "duration": 10027,
    "start_time": "2023-03-27T08:01:33.816Z"
   },
   {
    "duration": 56955,
    "start_time": "2023-03-27T08:11:44.957Z"
   },
   {
    "duration": 54,
    "start_time": "2023-03-27T08:13:43.099Z"
   },
   {
    "duration": 10,
    "start_time": "2023-03-27T08:16:58.928Z"
   },
   {
    "duration": 78752,
    "start_time": "2023-03-27T08:18:22.198Z"
   },
   {
    "duration": 406,
    "start_time": "2023-03-27T08:20:54.333Z"
   },
   {
    "duration": 40,
    "start_time": "2023-03-27T08:31:20.744Z"
   },
   {
    "duration": 13,
    "start_time": "2023-03-27T08:31:31.247Z"
   },
   {
    "duration": 4,
    "start_time": "2023-03-27T08:32:07.351Z"
   },
   {
    "duration": 6,
    "start_time": "2023-03-27T08:32:31.083Z"
   },
   {
    "duration": 5,
    "start_time": "2023-03-27T08:33:30.488Z"
   },
   {
    "duration": 18,
    "start_time": "2023-03-27T08:38:17.013Z"
   },
   {
    "duration": 22,
    "start_time": "2023-03-27T08:39:52.282Z"
   },
   {
    "duration": 126,
    "start_time": "2023-03-27T08:55:51.296Z"
   },
   {
    "duration": 4694,
    "start_time": "2023-03-27T08:56:47.048Z"
   },
   {
    "duration": 4440,
    "start_time": "2023-03-27T08:56:52.065Z"
   },
   {
    "duration": 155,
    "start_time": "2023-03-27T09:00:15.964Z"
   },
   {
    "duration": 4311,
    "start_time": "2023-03-27T09:01:36.342Z"
   },
   {
    "duration": 9,
    "start_time": "2023-03-27T09:03:27.759Z"
   },
   {
    "duration": 9,
    "start_time": "2023-03-27T09:03:35.159Z"
   },
   {
    "duration": 8,
    "start_time": "2023-03-27T09:04:55.024Z"
   },
   {
    "duration": 18,
    "start_time": "2023-03-27T09:04:58.128Z"
   },
   {
    "duration": 52,
    "start_time": "2023-03-27T09:08:22.636Z"
   },
   {
    "duration": 420,
    "start_time": "2023-03-27T09:08:46.765Z"
   },
   {
    "duration": 4,
    "start_time": "2023-03-27T09:09:49.661Z"
   },
   {
    "duration": 45,
    "start_time": "2023-03-27T09:29:11.213Z"
   },
   {
    "duration": 25,
    "start_time": "2023-03-27T09:36:22.503Z"
   },
   {
    "duration": 1373,
    "start_time": "2023-03-27T09:49:57.309Z"
   },
   {
    "duration": 732,
    "start_time": "2023-03-27T09:50:00.544Z"
   },
   {
    "duration": 44,
    "start_time": "2023-03-27T09:50:02.612Z"
   },
   {
    "duration": 224,
    "start_time": "2023-03-27T09:50:05.218Z"
   },
   {
    "duration": 9,
    "start_time": "2023-03-27T09:50:07.825Z"
   },
   {
    "duration": 6603,
    "start_time": "2023-03-27T09:50:12.912Z"
   },
   {
    "duration": 7,
    "start_time": "2023-03-27T09:50:23.737Z"
   },
   {
    "duration": 12,
    "start_time": "2023-03-27T09:50:26.537Z"
   },
   {
    "duration": 65,
    "start_time": "2023-03-27T09:50:29.345Z"
   },
   {
    "duration": 4,
    "start_time": "2023-03-27T09:50:31.144Z"
   },
   {
    "duration": 4,
    "start_time": "2023-03-27T09:50:33.945Z"
   },
   {
    "duration": 6,
    "start_time": "2023-03-27T09:51:14.415Z"
   },
   {
    "duration": 145,
    "start_time": "2023-03-27T09:51:17.005Z"
   },
   {
    "duration": 63,
    "start_time": "2023-03-27T09:51:19.580Z"
   },
   {
    "duration": 6,
    "start_time": "2023-03-27T09:51:22.354Z"
   },
   {
    "duration": 126,
    "start_time": "2023-03-27T09:51:24.227Z"
   },
   {
    "duration": 156,
    "start_time": "2023-03-27T09:51:47.851Z"
   },
   {
    "duration": 10351,
    "start_time": "2023-03-27T09:51:56.012Z"
   },
   {
    "duration": 57716,
    "start_time": "2023-03-27T09:52:13.652Z"
   },
   {
    "duration": 11368,
    "start_time": "2023-03-27T09:53:15.548Z"
   },
   {
    "duration": 63087,
    "start_time": "2023-03-27T09:53:37.011Z"
   },
   {
    "duration": 56,
    "start_time": "2023-03-27T10:00:11.937Z"
   },
   {
    "duration": 81659,
    "start_time": "2023-03-27T10:01:43.245Z"
   },
   {
    "duration": 424,
    "start_time": "2023-03-27T10:03:09.636Z"
   },
   {
    "duration": 19,
    "start_time": "2023-03-27T10:03:17.291Z"
   },
   {
    "duration": 6,
    "start_time": "2023-03-27T10:03:20.324Z"
   },
   {
    "duration": 1972,
    "start_time": "2023-03-28T07:21:50.596Z"
   },
   {
    "duration": 2556,
    "start_time": "2023-03-28T07:21:54.707Z"
   },
   {
    "duration": 46,
    "start_time": "2023-03-28T07:21:58.554Z"
   },
   {
    "duration": 2329,
    "start_time": "2023-03-28T07:22:32.715Z"
   },
   {
    "duration": 362,
    "start_time": "2023-03-28T07:22:37.252Z"
   },
   {
    "duration": 1157,
    "start_time": "2023-03-28T07:22:44.757Z"
   },
   {
    "duration": 52,
    "start_time": "2023-03-28T07:22:48.765Z"
   },
   {
    "duration": 278,
    "start_time": "2023-03-28T07:22:51.924Z"
   },
   {
    "duration": 16,
    "start_time": "2023-03-28T07:22:54.980Z"
   },
   {
    "duration": 951,
    "start_time": "2023-03-28T07:23:40.926Z"
   },
   {
    "duration": 40,
    "start_time": "2023-03-28T07:23:45.917Z"
   },
   {
    "duration": 624,
    "start_time": "2023-03-28T07:25:47.343Z"
   },
   {
    "duration": 1537,
    "start_time": "2023-03-28T07:26:06.657Z"
   },
   {
    "duration": 144,
    "start_time": "2023-03-28T07:28:47.139Z"
   },
   {
    "duration": 11,
    "start_time": "2023-03-28T07:28:58.546Z"
   },
   {
    "duration": 31,
    "start_time": "2023-03-28T07:29:50.850Z"
   },
   {
    "duration": 25,
    "start_time": "2023-03-28T07:29:56.483Z"
   },
   {
    "duration": 6,
    "start_time": "2023-03-28T07:29:59.467Z"
   },
   {
    "duration": 1200745,
    "start_time": "2023-03-28T07:30:01.364Z"
   },
   {
    "duration": 11,
    "start_time": "2023-03-28T07:59:17.529Z"
   },
   {
    "duration": 4,
    "start_time": "2023-03-28T07:59:33.673Z"
   },
   {
    "duration": 9,
    "start_time": "2023-03-28T08:00:16.419Z"
   },
   {
    "duration": 5,
    "start_time": "2023-03-28T08:01:16.619Z"
   },
   {
    "duration": 7,
    "start_time": "2023-03-28T08:01:18.811Z"
   },
   {
    "duration": 4,
    "start_time": "2023-03-28T08:03:05.981Z"
   },
   {
    "duration": 4,
    "start_time": "2023-03-28T08:03:43.989Z"
   },
   {
    "duration": 21,
    "start_time": "2023-03-28T08:04:14.110Z"
   },
   {
    "duration": 3,
    "start_time": "2023-03-28T08:10:24.093Z"
   },
   {
    "duration": 4990,
    "start_time": "2023-03-28T08:11:06.054Z"
   },
   {
    "duration": 1842,
    "start_time": "2023-03-29T05:21:46.820Z"
   },
   {
    "duration": 2921,
    "start_time": "2023-03-29T05:21:48.664Z"
   },
   {
    "duration": 49,
    "start_time": "2023-03-29T05:21:51.587Z"
   },
   {
    "duration": 245,
    "start_time": "2023-03-29T05:21:53.913Z"
   },
   {
    "duration": 11,
    "start_time": "2023-03-29T05:21:56.138Z"
   },
   {
    "duration": 93,
    "start_time": "2023-03-29T05:22:55.483Z"
   },
   {
    "duration": 113,
    "start_time": "2023-03-29T05:25:35.213Z"
   },
   {
    "duration": 31,
    "start_time": "2023-03-29T05:26:31.615Z"
   },
   {
    "duration": 663,
    "start_time": "2023-03-29T05:26:40.063Z"
   },
   {
    "duration": 122,
    "start_time": "2023-03-29T05:26:50.911Z"
   },
   {
    "duration": 1518,
    "start_time": "2023-03-29T05:27:18.303Z"
   },
   {
    "duration": 974,
    "start_time": "2023-03-29T05:27:20.822Z"
   },
   {
    "duration": 41,
    "start_time": "2023-03-29T05:27:23.934Z"
   },
   {
    "duration": 244,
    "start_time": "2023-03-29T05:27:27.551Z"
   },
   {
    "duration": 21,
    "start_time": "2023-03-29T05:27:29.575Z"
   },
   {
    "duration": 30,
    "start_time": "2023-03-29T05:27:40.735Z"
   },
   {
    "duration": 258,
    "start_time": "2023-03-29T05:27:42.952Z"
   },
   {
    "duration": 133,
    "start_time": "2023-03-29T05:27:52.015Z"
   },
   {
    "duration": 1463,
    "start_time": "2023-03-29T05:28:50.737Z"
   },
   {
    "duration": 5,
    "start_time": "2023-03-29T05:28:54.376Z"
   },
   {
    "duration": 4,
    "start_time": "2023-03-29T05:29:38.170Z"
   },
   {
    "duration": 11,
    "start_time": "2023-03-29T05:29:40.057Z"
   },
   {
    "duration": 4,
    "start_time": "2023-03-29T05:29:47.500Z"
   },
   {
    "duration": 4,
    "start_time": "2023-03-29T05:29:49.178Z"
   },
   {
    "duration": 11,
    "start_time": "2023-03-29T05:29:50.977Z"
   },
   {
    "duration": 108,
    "start_time": "2023-03-29T05:30:25.042Z"
   },
   {
    "duration": 57,
    "start_time": "2023-03-29T05:30:51.355Z"
   },
   {
    "duration": 1312952,
    "start_time": "2023-03-29T05:30:56.909Z"
   },
   {
    "duration": 9,
    "start_time": "2023-03-29T06:17:18.117Z"
   },
   {
    "duration": 3,
    "start_time": "2023-03-29T06:17:21.053Z"
   },
   {
    "duration": 4,
    "start_time": "2023-03-29T06:18:59.558Z"
   },
   {
    "duration": 4,
    "start_time": "2023-03-29T06:19:04.216Z"
   },
   {
    "duration": 4,
    "start_time": "2023-03-29T06:19:08.335Z"
   },
   {
    "duration": 7,
    "start_time": "2023-03-29T06:31:28.655Z"
   },
   {
    "duration": 115,
    "start_time": "2023-03-29T06:33:00.343Z"
   },
   {
    "duration": 30,
    "start_time": "2023-03-29T06:33:09.048Z"
   },
   {
    "duration": 85,
    "start_time": "2023-03-29T06:33:10.951Z"
   },
   {
    "duration": 7,
    "start_time": "2023-03-29T06:33:14.784Z"
   },
   {
    "duration": 5,
    "start_time": "2023-03-29T06:33:16.810Z"
   },
   {
    "duration": 16,
    "start_time": "2023-03-29T06:33:19.960Z"
   },
   {
    "duration": 139,
    "start_time": "2023-03-29T06:36:21.914Z"
   },
   {
    "duration": 133,
    "start_time": "2023-03-29T06:38:29.111Z"
   },
   {
    "duration": 234,
    "start_time": "2023-03-29T06:40:27.096Z"
   },
   {
    "duration": 90,
    "start_time": "2023-03-29T06:45:16.125Z"
   },
   {
    "duration": 10,
    "start_time": "2023-03-29T06:46:32.648Z"
   },
   {
    "duration": 8,
    "start_time": "2023-03-29T06:46:44.206Z"
   },
   {
    "duration": 4,
    "start_time": "2023-03-29T06:46:50.175Z"
   },
   {
    "duration": 4,
    "start_time": "2023-03-29T06:47:03.376Z"
   },
   {
    "duration": 41,
    "start_time": "2023-03-29T06:47:18.073Z"
   },
   {
    "duration": 1493367,
    "start_time": "2023-03-29T06:49:43.427Z"
   },
   {
    "duration": 4992238,
    "start_time": "2023-03-29T07:46:04.997Z"
   },
   {
    "duration": 3,
    "start_time": "2023-03-29T09:15:09.969Z"
   },
   {
    "duration": 30804,
    "start_time": "2023-03-29T09:15:18.627Z"
   },
   {
    "duration": 65251,
    "start_time": "2023-03-29T09:16:47.050Z"
   },
   {
    "duration": 28,
    "start_time": "2023-03-29T09:18:21.571Z"
   },
   {
    "duration": 22,
    "start_time": "2023-03-29T09:18:43.732Z"
   },
   {
    "duration": 16,
    "start_time": "2023-03-29T09:18:50.331Z"
   }
  ],
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Содержание",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "302.391px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
